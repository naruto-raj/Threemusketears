{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping /Users/naveenrajg/Desktop/Doc/Encode/stab/lib/python3.12/site-packages/accelerate-0.27.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain in ./stab/lib/python3.12/site-packages (0.1.11)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./stab/lib/python3.12/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./stab/lib/python3.12/site-packages (from langchain) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./stab/lib/python3.12/site-packages (from langchain) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./stab/lib/python3.12/site-packages (from langchain) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./stab/lib/python3.12/site-packages (from langchain) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in ./stab/lib/python3.12/site-packages (from langchain) (0.0.27)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.29 in ./stab/lib/python3.12/site-packages (from langchain) (0.1.30)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./stab/lib/python3.12/site-packages (from langchain) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./stab/lib/python3.12/site-packages (from langchain) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./stab/lib/python3.12/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./stab/lib/python3.12/site-packages (from langchain) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./stab/lib/python3.12/site-packages (from langchain) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./stab/lib/python3.12/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./stab/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./stab/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./stab/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./stab/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./stab/lib/python3.12/site-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./stab/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./stab/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./stab/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\n",
      "\u001b[33mWARNING: Skipping /Users/naveenrajg/Desktop/Doc/Encode/stab/lib/python3.12/site-packages/accelerate-0.27.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping /Users/naveenrajg/Desktop/Doc/Encode/stab/lib/python3.12/site-packages/accelerate-0.27.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: langchain_experimental in ./stab/lib/python3.12/site-packages (0.0.53)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.8 in ./stab/lib/python3.12/site-packages (from langchain_experimental) (0.1.11)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.27 in ./stab/lib/python3.12/site-packages (from langchain_experimental) (0.1.30)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.0.28)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.4)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.27)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (0.1.23)\n",
      "Requirement already satisfied: numpy<2,>=1 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3,>=1 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.6.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in ./stab/lib/python3.12/site-packages (from langchain<0.2.0,>=0.1.8->langchain_experimental) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in ./stab/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (4.3.0)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in ./stab/lib/python3.12/site-packages (from langchain-core<0.2.0,>=0.1.27->langchain_experimental) (23.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in ./stab/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.9.4)\n",
      "Requirement already satisfied: idna>=2.8 in ./stab/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (3.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./stab/lib/python3.12/site-packages (from anyio<5,>=3->langchain-core<0.2.0,>=0.1.27->langchain_experimental) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./stab/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.21.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in ./stab/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./stab/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./stab/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.9.15)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in ./stab/lib/python3.12/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.8->langchain_experimental) (4.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./stab/lib/python3.12/site-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.8->langchain_experimental) (2024.2.2)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./stab/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.8->langchain_experimental) (1.0.0)\n",
      "\u001b[33mWARNING: Skipping /Users/naveenrajg/Desktop/Doc/Encode/stab/lib/python3.12/site-packages/accelerate-0.27.2.dist-info due to invalid metadata entry 'name'\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install langchain_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    ")\n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_experimental.chat_models import Llama2Chat\n",
    "from os.path import expanduser\n",
    "\n",
    "from langchain_community.llms import LlamaCpp\n",
    "import re\n",
    "\n",
    "def lang_chains():\n",
    "    template_message1 = [\n",
    "        SystemMessage(content=\"You are the narrator of a dungeons and dragon game and you assist the player play the game based on the players responses, you ll ask the player to select suggested options answer\"),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    "    prompt_template1 = ChatPromptTemplate.from_messages(template_message1)\n",
    "    template_message2 = [\n",
    "        SystemMessage(content=\"You are the narrator of a dungeons and dragon game and assist in generating a text prompt for an image generation model for the given scenario\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{text}\"),\n",
    "    ]\n",
    "    prompt_template2 = ChatPromptTemplate.from_messages(template_message2)\n",
    "    model_path1 = \"/Users/naveenrajg/Downloads/llama-2-7b-chat.Q2_K.gguf\"\n",
    "    model_path2 = \"/Users/naveenrajg/Downloads/llama-2-7b-chat.Q2_K.gguf\"\n",
    "\n",
    "    llm1 = LlamaCpp(\n",
    "        model_path=model_path1,\n",
    "        streaming=False,\n",
    "        n_ctx=2048,\n",
    "    )\n",
    "    llm2 = LlamaCpp(\n",
    "        model_path=model_path2,\n",
    "        streaming=False,\n",
    "        n_ctx=512,\n",
    "    )\n",
    "    model1 = Llama2Chat(llm=llm1)\n",
    "    model2 = Llama2Chat(llm=llm2)\n",
    "    memory1 = ConversationBufferMemory(memory_key=\"chat_history\", k=2, return_messages=True)\n",
    "    memory2 = ConversationBufferMemory(memory_key=\"prompt_history\", k=2, return_messages=True)\n",
    "    chain1 = LLMChain(llm=model1, prompt=prompt_template1, memory=memory1)\n",
    "    chain2 = LLMChain(llm=model2, prompt=prompt_template2, memory=memory2)\n",
    "    return chain1,chain2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_prefix(text):\n",
    "    return text + \" . create a sentence prompt to generate an image for this scene in format {prompt:'prompt'}\"\n",
    "\n",
    "def extract_prompt(text):\n",
    "    \"\"\"\n",
    "    Extracts the prompt from the given text.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text containing the prompt.\n",
    "        \n",
    "    Returns:\n",
    "        str: The extracted prompt, or None if no prompt is found.\n",
    "    \"\"\"\n",
    "    # Define a regular expression pattern to match the prompt\n",
    "    pattern = r'\"(.*?)\"'\n",
    "    \n",
    "    # Search for the pattern in the text\n",
    "    match = re.search(pattern, text)\n",
    "    \n",
    "    if match:\n",
    "        # If a match is found, return the prompt (group 1 of the match)\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # If no match is found, return None\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_chat(text):\n",
    "    image_prompt = chain2.run(\n",
    "        text=add_prefix(text)\n",
    "    )\n",
    "\n",
    "    story = chain1.run(\n",
    "            text=text\n",
    "        )\n",
    "    return image_prompt,story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A hungry Knight enters the realm of dragons . create a sentence prompt to generate an image for this scene in format {prompt:'prompt'}\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_prefix('A hungry Knight enters the realm of dragons')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/naveenrajg/Downloads/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.20 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     2.50 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '10', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /Users/naveenrajg/Downloads/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q2_K:   65 tensors\n",
      "llama_model_loader: - type q3_K:  160 tensors\n",
      "llama_model_loader: - type q6_K:    1 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.11 MiB\n",
      "llm_load_tensors: offloading 0 repeating layers to GPU\n",
      "llm_load_tensors: offloaded 0/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =  2694.32 MiB\n",
      ".................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_new_context_with_model:        CPU input buffer size   =     0.14 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.10 MiB\n",
      "llama_new_context_with_model: graph splits (measure): 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.model': 'llama', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.dimension_count': '128', 'general.file_type': '10', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using fallback chat format: None\n"
     ]
    }
   ],
   "source": [
    "chain1,chain2 = lang_chains()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/naveenrajg/Desktop/Doc/Encode/stab/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "\n",
      "llama_print_timings:        load time =    3344.93 ms\n",
      "llama_print_timings:      sample time =      25.06 ms /   236 runs   (    0.11 ms per token,  9415.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8523.89 ms /    84 tokens (  101.47 ms per token,     9.85 tokens per second)\n",
      "llama_print_timings:        eval time =   17181.48 ms /   235 runs   (   73.11 ms per token,    13.68 tokens per second)\n",
      "llama_print_timings:       total time =   26099.85 ms /   319 tokens\n",
      "\n",
      "llama_print_timings:        load time =     991.09 ms\n",
      "llama_print_timings:      sample time =      14.56 ms /   192 runs   (    0.08 ms per token, 13182.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4895.45 ms /    71 tokens (   68.95 ms per token,    14.50 tokens per second)\n",
      "llama_print_timings:        eval time =   12931.38 ms /   191 runs   (   67.70 ms per token,    14.77 tokens per second)\n",
      "llama_print_timings:       total time =   18135.79 ms /   262 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A brave knight rides his loyal steed into the mystical realm of dragons, his armor gleaming in the fading sunlight as he prepares to face the fire-breathing beasts that lurk within.\n",
      "  Ah, a brave adventurer has entered the realm of dragons! As their trusty narrator, I will guide them through this fantastical world filled with mythical creatures.\n",
      "The Knight approaches a bend in the path, and as they round it, they see a magnificent dragon standing in their path. The dragon's scales glint in the sunlight, casting a warm glow over the landscape.\n",
      "To continue the adventure, you have the following options:\n",
      "1. Approach the dragon cautiously, ready for any unexpected movements.\n",
      "2. Hail the dragon and attempt to strike up a conversation.\n",
      "3. Attempt to sneak past the dragon, hoping to go unnoticed.\n",
      "4. Charge towards the dragon with weapon drawn, ready to take it down.\n",
      "Please select one of these options by typing the corresponding number.\n"
     ]
    }
   ],
   "source": [
    "image_prompt,story = LLM_chat(\"A hungry Knight enters the realm of dragons\")\n",
    "print(extract_prompt(image_prompt))\n",
    "print(story)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    2608.11 ms\n",
      "llama_print_timings:      sample time =      11.86 ms /   153 runs   (    0.08 ms per token, 12901.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4272.12 ms /    68 tokens (   62.83 ms per token,    15.92 tokens per second)\n",
      "llama_print_timings:        eval time =   10087.66 ms /   152 runs   (   66.37 ms per token,    15.07 tokens per second)\n",
      "llama_print_timings:       total time =   14604.49 ms /   220 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     679.95 ms\n",
      "llama_print_timings:      sample time =      17.40 ms /   230 runs   (    0.08 ms per token, 13219.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11588.87 ms /   187 tokens (   61.97 ms per token,    16.14 tokens per second)\n",
      "llama_print_timings:        eval time =   15989.82 ms /   229 runs   (   69.82 ms per token,    14.32 tokens per second)\n",
      "llama_print_timings:       total time =   27970.55 ms /   416 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A narrow path winds through the dense underbrush, offering a shadowy refuge from the scorching sun. The air is thick with the pungent scent of wet earth and decaying vegetation, and the rustling of small creatures can be heard in the darkness. As you make your way deeper into the thicket, the trees grow taller and closer together, their branches tangling overhead like grasping fingers. The light grows dimmer, and the shadows seem to move and twist, as if alive.\n",
      "  Ah, a wise choice, my brave adventurer! *chuckles* Taking a detour through the bushes will indeed increase your chances of avoiding any potential danger lurking in the underbrush. *winks*\n",
      "As you make your way through the dense foliage, the sounds of rustling leaves and snapping twigs fill the air. You can't help but feel a sense of unease as you navigate through the narrow path. The bushes seem to stretch on forever, blocking any clear path to your destination.\n",
      "Just as you're starting to lose hope, you hear a faint rumbling in the distance. The sound grows louder and louder until...\n",
      "Do you:\n",
      "Option 1: Keep moving forward, trying to find the source of the rumbling noise.\n",
      "Option 2: Hide behind a nearby tree, waiting for whatever is making that noise to come to you.\n",
      "Option 3: Go back the way you came, back to the safety of your camp.\n",
      "What will you do, brave adventurer?\n"
     ]
    }
   ],
   "source": [
    "image_prompt,story = LLM_chat(\"Take a short detour through the bushes to avoid any potential danger that may be lurking in the underbrush. This route is more indirect, but it may be safer in the long run.\")\n",
    "print(extract_prompt(image_prompt))\n",
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
